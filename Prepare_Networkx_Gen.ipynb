{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import MetaData , Table, Column, Integer, Numeric, String, DateTime, ForeignKey, Text\n",
    "from sqlalchemy import select, desc\n",
    "from sqlalchemy import and_, or_, not_\n",
    "from sqlalchemy import text\n",
    "\n",
    "# engine = create_engine('postgresql+psycopg2://@hdpedge001.ca.sunlife:5432/podium_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import datetime as dt\n",
    "import os\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OUTPUT_DIR = r'C:\\Projects\\CEL\\Dataflows\\Extracts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(user, password, db, host='hdpedge001.ca.sunlife', port=5432):\n",
    "    '''Returns a connection and a metadata object'''\n",
    "    \n",
    "    url = 'postgresql+psycopg2://{}:{}@{}:{}/{}'\n",
    "    url = url.format(user, password, host, port, db)\n",
    "\n",
    "    # The return value of create_engine() is our connection object\n",
    "    con = create_engine(url, client_encoding='utf8')\n",
    "\n",
    "    # We then bind the connection to MetaData()\n",
    "    meta = MetaData(bind=con)\n",
    "\n",
    "    return con, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_join_condition(con, meta, joiner_nid):\n",
    "    \n",
    "    pd_prep_joiner_condition = Table('podium_core.pd_prep_joiner_condition', meta)\n",
    "    \n",
    "    s = pd_prep_joiner_condition.select()\n",
    "    s = s.where(pd_prep_joiner_condition.c.joiner_nid == joiner_nid)\n",
    "    s = s.order_by(pd_prep_joiner_condition.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_join_master(con, meta, joiner_nid):\n",
    "    \n",
    "    pd_prep_joiner_master_field = Table('podium_core.pd_prep_joiner_master_field', meta)\n",
    "    \n",
    "    s = pd_prep_joiner_master_field.select()\n",
    "    s = s.where(pd_prep_joiner_master_field.c.joiner_nid == joiner_nid)\n",
    "    s = s.order_by(pd_prep_joiner_master_field.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_join_detail(con, meta, joiner_nid):\n",
    "    \n",
    "    pd_prep_joiner_detail_field = Table('podium_core.pd_prep_joiner_detail_field', meta)\n",
    "    \n",
    "    s = pd_prep_joiner_detail_field.select()\n",
    "    s = s.where(pd_prep_joiner_detail_field.c.joiner_nid == joiner_nid)\n",
    "    s = s.order_by(pd_prep_joiner_detail_field.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trans_expression(con, meta, transformer_nid):\n",
    "    \n",
    "    pd_prep_transformer_expression = Table('podium_core.pd_prep_transformer_expression', meta)\n",
    "    \n",
    "    s = pd_prep_transformer_expression.select()\n",
    "    s = s.where(pd_prep_transformer_expression.c.transformer_nid == transformer_nid)\n",
    "    s = s.order_by(pd_prep_transformer_expression.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregator_expression(con, meta, aggregator_nid):\n",
    "    \n",
    "    pd_prep_aggregator_expression = Table('podium_core.pd_prep_aggregator_expression', meta)\n",
    "    \n",
    "    s = pd_prep_aggregator_expression.select()\n",
    "    s = s.where(pd_prep_aggregator_expression.c.aggregator_nid == aggregator_nid)\n",
    "    s = s.order_by(pd_prep_aggregator_expression.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregator_group(con, meta, aggregator_nid):\n",
    "    \n",
    "    pd_prep_aggregator_group = Table('podium_core.pd_prep_aggregator_group', meta)\n",
    "    \n",
    "    s = pd_prep_aggregator_group.select()\n",
    "    s = s.where(pd_prep_aggregator_group.c.aggregator_nid == aggregator_nid)\n",
    "    s = s.order_by(pd_prep_aggregator_group.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_condition(con, meta, sorter_nid):\n",
    "    \n",
    "    pd_prep_sort_condition = Table('podium_core.pd_prep_sort_condition', meta)\n",
    "    \n",
    "    s = pd_prep_sort_condition.select()\n",
    "    s = s.where(pd_prep_sort_condition.c.sorter_nid == sorter_nid)\n",
    "    s = s.order_by(pd_prep_sort_condition.c.idx)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_expressions(con, meta, router_nid):\n",
    "    \n",
    "    pd_prep_router_expression = Table('podium_core.pd_prep_router_expression', meta)\n",
    "    \n",
    "    rtr = pd_prep_router_expression.select()\n",
    "    rtr = rtr.where(pd_prep_router_expression.c.router_nid == router_nid)\n",
    "    rtr = rtr.order_by(pd_prep_router_expression.c.idx)\n",
    "    \n",
    "    rp = con.execute(rtr)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_expression(con, meta, custom_nid):\n",
    "    \n",
    "    pd_custom_expression = Table('podium_core.pd_prep_package', meta)\n",
    "    \n",
    "    cust = pd_custom_expression.select()\n",
    "    cust = cust.where(pd_custom_expression.nid == custom_nid)\n",
    "    \n",
    "    rp = con.execute(cust)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compare_condition(con, meta, compare_nid):\n",
    "    \n",
    "    pd_prep_compare_condition = Table('podium_core.pd_prep_compare_condition', meta)\n",
    "    \n",
    "    cmp = pd_prep_compare_condition.select()\n",
    "    cmp = cmp.where(pd_prep_compare_condition.c.compare_nid == compare_nid)\n",
    "    cmp = cmp.order_by(pd_prep_compare_condition.c.idx)\n",
    "    \n",
    "    rp = con.execute(cmp)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compare_join(con, meta, compare_nid):\n",
    "    \n",
    "    pd_prep_compare_join = Table('podium_core.pd_prep_compare_join', meta)\n",
    "    \n",
    "    cmj = pd_prep_compare_join.select()\n",
    "    cmj = cmj.where(pd_prep_compare_join.c.compare_nid == compare_nid)\n",
    "    cmj = cmj.order_by(pd_prep_compare_join.c.idx)\n",
    "    \n",
    "    rp = con.execute(cmj)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(con, meta, source_nid):\n",
    "    \n",
    "    pd_source = Table('podium_core.pd_source', meta)\n",
    "    \n",
    "    s = pd_source.select()\n",
    "    s = s.where(pd_source.c.nid == source_nid)\n",
    "    \n",
    "    r = con.execute(s)\n",
    "    \n",
    "    return r.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(con, meta, entity_nid):\n",
    "    \"\"\"Fetches the entity record, returns 1 row at most\"\"\"\n",
    "    \n",
    "    pd_entity = Table('podium_core.pd_entity', meta)\n",
    "    \n",
    "    s = select([pd_entity.c.sname,\n",
    "                pd_entity.c.source_nid])\n",
    "    s = s.where(pd_entity.c.nid == entity_nid)\n",
    "    \n",
    "    r = con.execute(s)\n",
    "    \n",
    "    return r.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_id(con, meta, sname):\n",
    "    \"\"\"Get the bundle id of the passed Prepare Workflow name.\n",
    "    \n",
    "    This is a case sensitive match and can only return a single row\n",
    "    or None\n",
    "    \"\"\"\n",
    "    pd_bundle = Table('podium_core.pd_bundle', meta)\n",
    "    \n",
    "    sname = sname.lower()\n",
    "    \n",
    "    s = pd_bundle.select()\n",
    "    s = s.where(text(\"lower(sname) = :lc_sname\"))\n",
    "    \n",
    "    rp = con.execute(s, lc_sname=sname)\n",
    "    \n",
    "    r = rp.first()\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_gui_state(con, meta, nid):\n",
    "    \"\"\"Get the bundle id of the passes Prepare Workflow name.\n",
    "    \n",
    "    This is a case sensitive match and can only return a single row\n",
    "    or None\n",
    "    \"\"\"\n",
    "    pd_bundle_gui_state = Table('podium_core.pd_bundle_gui_state', meta)\n",
    "    \n",
    "    s = pd_bundle_gui_state.select()\n",
    "    s = s.where(pd_bundle_gui_state.c.nid == nid)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.first()\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_last_execution(con, meta, bundle_nid, count=10):\n",
    "    \"\"\"Get the last count execution details of the specified bundle.\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_prepare_execution_workorder = Table('podium_core.pd_prepare_execution_workorder', meta)\n",
    "    \n",
    "    # e = pd_prepare_execution_workorder.select()\n",
    "    e = select([pd_prepare_execution_workorder.c.nid,\n",
    "               pd_prepare_execution_workorder.c.record_count,\n",
    "               pd_prepare_execution_workorder.c.start_time,\n",
    "               pd_prepare_execution_workorder.c.end_time])\n",
    "    e = e.where(and_(pd_prepare_execution_workorder.c.bundle_nid == bundle_nid, \n",
    "                     pd_prepare_execution_workorder.c.end_time.isnot(None),\n",
    "                     pd_prepare_execution_workorder.c.workorder_status == \"FINISHED\"))\n",
    "    e = e.order_by(desc(pd_prepare_execution_workorder.c.end_time))\n",
    "    e = e.limit(count)\n",
    "    \n",
    "    # print(str(e))\n",
    "    \n",
    "    rp = con.execute(e)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_last_load(con, meta, source_nid, entity_name, n=1):\n",
    "    \"\"\"Get the last execution details of the specified bundle.\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_source = Table('podium_core.pd_source', meta)\n",
    "    pd_entity = Table('podium_core.pd_entity', meta)\n",
    "    pd_workorder = Table('podium_core.pd_workorder', meta)\n",
    "    \n",
    "    #print entity_name\n",
    "    \n",
    "    parent_source = get_source(con, meta, source_nid)\n",
    "    \n",
    "    src = pd_source.select()\n",
    "    src = src.where(pd_source.c.sname == parent_source.sname)\n",
    "    \n",
    "    srp = con.execute(src)\n",
    "    \n",
    "    orig_source_id = None\n",
    "    \n",
    "    for r in srp:\n",
    "        print(\"Source: {}, Source Type: {}, nid: {}\".format(r.sname, r.source_type, r.nid ))\n",
    "        if r.source_type != 'PODIUM_INTERNAL':\n",
    "            orig_source_id = r.nid\n",
    "            break\n",
    "    \n",
    "    #print r\n",
    "    print(\"orig_source_id: {}\".format(orig_source_id))\n",
    "    \n",
    "    if orig_source_id is None:\n",
    "        return None\n",
    "    \n",
    "    ety = pd_entity.select()\n",
    "    ety = ety.where(and_(pd_entity.c.source_nid == orig_source_id, pd_entity.c.sname == entity_name))\n",
    "\n",
    "    erp = con.execute(ety)\n",
    "    \n",
    "    orig_entity = erp.first()\n",
    "    \n",
    "    \n",
    "    if orig_entity is not None:\n",
    "        \n",
    "        orig_entity_nid = orig_entity.nid\n",
    "\n",
    "        wo = select([pd_workorder.c.nid,\n",
    "                     pd_workorder.c.start_time,\n",
    "                     pd_workorder.c.end_time,\n",
    "                     pd_workorder.c.record_count,\n",
    "                     pd_workorder.c.good_count,\n",
    "                     pd_workorder.c.bad_count,\n",
    "                     pd_workorder.c.ugly_count])\n",
    "\n",
    "        wo = wo.where(and_(pd_workorder.c.entity_nid == orig_entity_nid, pd_workorder.c.workorder_status == 'FINISHED'))\n",
    "        wo = wo.order_by(desc(pd_workorder.c.end_time))\n",
    "        wo = wo.limit(n) \n",
    "\n",
    "        rp = con.execute(wo)\n",
    "\n",
    "        r = rp.first()\n",
    "    \n",
    "    else:\n",
    "        r = None\n",
    "    \n",
    "    #print r\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_formats(con, meta, package_nid):\n",
    "    \"\"\"Get the inbound / outbound field formats for the passed package_nid\n",
    "    \n",
    "    Returns a dictionary of the form\n",
    "    \n",
    "    {INBOUND:[[],[],...],\n",
    "     OUTBOUND:[[],[],...]}\n",
    "     \n",
    "     Typically the OUTBOUND list would have only 1 member list.\n",
    "    \"\"\"\n",
    "    package_field_formats = {'INBOUND':[], 'OUTBOUND':[]}\n",
    "    \n",
    "    pd_prep_port = Table('podium_core.pd_prep_port', meta)\n",
    "    pd_prep_field_format = Table('podium_core.pd_prep_field_format', meta)\n",
    "    \n",
    "    #for c in pd_prep_package.c:\n",
    "    #    print c\n",
    "    \n",
    "    s = pd_prep_port.select()\n",
    "    s = s.where(pd_prep_port.c.package_nid == package_nid)\n",
    "\n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    for p in rp:\n",
    "        \n",
    "        record_format_nid = p.record_format_nid\n",
    "        f = pd_prep_field_format.select()\n",
    "        f = f.where(pd_prep_field_format.c.record_format_nid == record_format_nid)\n",
    "        f = f.order_by(pd_prep_field_format.c.idx)\n",
    "        \n",
    "        fp = con.execute(f)\n",
    "        \n",
    "        field_formats = fp.fetchall()\n",
    "        \n",
    "        # Convert to list of lists\n",
    "        ff = [list(f) for f in field_formats]\n",
    "        \n",
    "        if p.port_type == 'INBOUND':\n",
    "            package_field_formats['INBOUND'].append(ff)\n",
    "        else:\n",
    "            package_field_formats['OUTBOUND'].append(ff)\n",
    "            \n",
    "        fp.close()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return package_field_formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_package_nodes(con, meta, bundle_nid):\n",
    "    \n",
    "    pd_prep_package = Table('podium_core.pd_prep_package', meta)\n",
    "    \n",
    "    #for c in pd_prep_package.c:\n",
    "    #    print c\n",
    "    \n",
    "    s = pd_prep_package.select()\n",
    "    s = s.where(pd_prep_package.c.bundle_nid == bundle_nid)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_package_connectors(con, meta, bundle_nid, G):\n",
    "    \"\"\"Get all the connectors between the various transforms.\n",
    "    \n",
    "    There is a connecter between a node and its source data (inbound data).\n",
    "    \n",
    "    This function constructs a list of (from, to) tuples that describe the\n",
    "    DiGraph edges and their direction.\n",
    "    \"\"\"\n",
    "    \n",
    "    p_edges = []\n",
    "    \n",
    "    OP_TEMPLATE = \"From: {}, To: {}, ob_idx: {}, Outbound: {}, ib_idx: {}, Inbound: {}\"\n",
    "    \n",
    "    pd_prep_connector = Table('podium_core.pd_prep_connector', meta)\n",
    "    pd_prep_port = Table('podium_core.pd_prep_port', meta)\n",
    "    pd_prep_package = Table('podium_core.pd_prep_package', meta)\n",
    "    \n",
    "    s = pd_prep_connector.select()\n",
    "    s = s.where(pd_prep_connector.c.bundle_nid == bundle_nid)\n",
    "    \n",
    "    c_rp = con.execute(s)\n",
    "    \n",
    "    # con_rows = c_rp.fetchall()\n",
    "    \n",
    "    for c in c_rp:\n",
    "        \n",
    "        ob_port = c.outbound_port_nid\n",
    "        ib_port = c.inbound_port_nid\n",
    "        \n",
    "        p = pd_prep_port.select()\n",
    "        p = p.where(pd_prep_port.c.nid.in_([ob_port, ib_port]))\n",
    "        \n",
    "        p_rp = con.execute(p)\n",
    "        p_rows = p_rp.fetchall()\n",
    "        \n",
    "        # node outbound port -> connector outbound port -> connector inbound port -> node inbound port\n",
    "        for port in p_rows:\n",
    "            \n",
    "            #pkg = pd_prep_package.select()\n",
    "            #pkg = pkg.where(pd_prep_package.c.nid == port.package_nid)\n",
    "            #pkg_rp = con.execute(pkg)\n",
    "            #pkg_rows = pkg_rp.fetchall()\n",
    "            \n",
    "            #for package in pkg_rows:\n",
    "            #    package_type = package.package_type\n",
    "            package_type = G.node[port.package_nid]['n_type']\n",
    "            \n",
    "            e_type = port.port_type\n",
    "            \n",
    "            if e_type == 'OUTBOUND':\n",
    "                e_from_nid = port.package_nid\n",
    "                e_ob_idx = port.idx\n",
    "                e_outbound_type = package_type\n",
    "            elif e_type == 'INBOUND':\n",
    "                e_to_nid = port.package_nid\n",
    "                e_ib_idx = port.idx\n",
    "                e_inbound_type = package_type\n",
    "        \n",
    "        p_rp.close()\n",
    "        \n",
    "        print(OP_TEMPLATE.format(e_from_nid, e_to_nid, e_ob_idx, e_outbound_type, e_ib_idx, e_inbound_type))\n",
    "        \n",
    "        p_edges.append((e_from_nid, e_to_nid, e_ob_idx, e_outbound_type, e_ib_idx, e_inbound_type))\n",
    "\n",
    "    c_rp.close()\n",
    "    \n",
    "    return p_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This closure function us used to keep a records of\n",
    "# the max y values at a level in the graph\n",
    "def make_free():\n",
    "    free = {}\n",
    "    def get_free(x):\n",
    "        nonlocal free\n",
    "        if x in free:\n",
    "            free[x] = free[x] + 10\n",
    "        else:\n",
    "            free[x] = 1\n",
    "        return free[x]\n",
    "    return get_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_level(G, node_id, free, curr_level, curr_free):\n",
    "    \"\"\"Set the level of the node for use in the visjs Network\"\"\"\n",
    "    \n",
    "    if G.node[node_id]['level'] < curr_level:\n",
    "        G.node[node_id]['level'] = curr_level\n",
    "        \n",
    "    G.node[node_id]['level_free'] = curr_free\n",
    "    \n",
    "    print(\"node_id: {}, curr_level: {}, curr_free: {}\".format(node_id, curr_level, curr_free))\n",
    "    curr_level += 1\n",
    "    \n",
    "    p_predecessors = G.predecessors(node_id)\n",
    "    \n",
    "    for e, pre in enumerate(p_predecessors):\n",
    "        curr_free = free(curr_level)\n",
    "        set_level(G, pre, free, curr_level, curr_free)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_nodes(con, meta, G, bundle_package):\n",
    "    \"\"\"Create a new DiGraph from the package nodes\n",
    "    \n",
    "    Node data attributes / node type (n_type):\n",
    "    \n",
    "       n_type\n",
    "       \n",
    "       LOADER / STORE\n",
    "       --------------\n",
    "       entity_id\n",
    "       data_selection\n",
    "       source_id\n",
    "       source_name\n",
    "       \n",
    "       JOINER\n",
    "       ------\n",
    "       join_condition\n",
    "       \n",
    "       TRANSFORMER\n",
    "       -----------\n",
    "       \n",
    "       AGGREGATOR\n",
    "       ----------\n",
    "       aggregator_expression\n",
    "       aggregator_group\n",
    "       \n",
    "       SORTER\n",
    "       ------\n",
    "       sort-condition\n",
    "       \n",
    "       FILTER\n",
    "       ------\n",
    "       filter-expression\n",
    "       \n",
    "       UNION\n",
    "       -----\n",
    "       union_type\n",
    "       \n",
    "       ROUTER\n",
    "       ------\n",
    "       router_expressions\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    node_count = len(bundle_package)\n",
    "    \n",
    "    print(\"{:d} nodes in \".format(node_count))\n",
    "    \n",
    "    #pd_entity = Table('podium_core.pd_entity', meta)\n",
    "    #pd_source = Table('podium_core.pd_source', meta)\n",
    "    \n",
    "    sources = {}\n",
    "    \n",
    "    for row in bundle_package:\n",
    "        \n",
    "        id = row.nid\n",
    "        \n",
    "        n_type = row.package_type\n",
    "        \n",
    "        print(\"{:4d} {}\".format(id, n_type))\n",
    "        \n",
    "        G.add_node(id)\n",
    "        \n",
    "        G.node[id]['n_type'] = n_type\n",
    "        G.node[id]['comments'] = row.package_comments\n",
    "        G.node[id]['field_formats'] = get_field_formats(con, meta, id)\n",
    "        G.node[id]['level'] = 0\n",
    "    \n",
    "        if n_type in ('LOADER','STORE'):\n",
    "            \n",
    "            entity_id = row.entity_id\n",
    "            G.node[id]['entity_id'] = entity_id\n",
    "            G.node[id]['data_selection'] = row.data_selection\n",
    "            \n",
    "            entity = get_entity(con, meta, row.entity_id)\n",
    "                \n",
    "            entity_name = entity.sname\n",
    "            source_id = entity.source_nid\n",
    "            \n",
    "            # to-do Tie workflow entity (INTERNAL) back to source\n",
    "            if n_type == 'LOADER':                \n",
    "                workorder = get_entity_last_load(con, meta, source_id, entity_name)\n",
    "            else:\n",
    "                workorder = None\n",
    "                \n",
    "            G.node[id]['entity_name'] = entity_name\n",
    "            G.node[id]['source_id'] = source_id \n",
    "            \n",
    "            if workorder:\n",
    "                G.node[id]['start_time'] = str(workorder.start_time)\n",
    "                G.node[id]['end_time'] = str(workorder.end_time)\n",
    "                G.node[id]['record_count'] = workorder.record_count\n",
    "                G.node[id]['good_count'] = workorder.good_count\n",
    "                G.node[id]['bad_count'] = workorder.bad_count\n",
    "                G.node[id]['ugly_count'] = workorder.ugly_count\n",
    "            else:\n",
    "                G.node[id]['record_count'] = 0\n",
    "                G.node[id]['good_count'] = 0\n",
    "                G.node[id]['bad_count'] = 0\n",
    "                G.node[id]['ugly_count'] = 0\n",
    "                \n",
    "            if source_id in sources:\n",
    "                pass\n",
    "            else:\n",
    "                parent_source = get_source(con, meta, source_id)\n",
    "                source_name = parent_source.sname\n",
    "                    \n",
    "                sources[source_id] = {'sname':source_name}\n",
    "                \n",
    "            source_name = sources[source_id]['sname']\n",
    "                \n",
    "            G.node[id]['source_name'] = source_name\n",
    "                \n",
    "        elif n_type == 'JOINER':\n",
    "            \n",
    "            G.node[id]['join_type'] = row.mode\n",
    "            \n",
    "            j_rows = get_join_condition(con, meta, id)\n",
    "            \n",
    "            join_condition = []\n",
    "            \n",
    "            for j in j_rows:\n",
    "                idx = j.idx\n",
    "                join_condition.append((j.master_expression, j.join_operator, j.detail_expression))\n",
    "            \n",
    "            G.node[id]['join_condition'] = join_condition\n",
    "            \n",
    "            master_fields = []\n",
    "            \n",
    "            m_rows = get_join_master(con, meta, id)\n",
    "            \n",
    "            for m in m_rows:\n",
    "                master_fields.append((m.field_name, m.idx))\n",
    "             \n",
    "            G.node[id]['master_fields'] = master_fields\n",
    "                \n",
    "            detail_fields = []\n",
    "            \n",
    "            d_rows = get_join_detail(con, meta, id)\n",
    "            \n",
    "            for d in d_rows:\n",
    "                detail_fields.append((d.field_name, d.idx))\n",
    "             \n",
    "            G.node[id]['detail_fields'] = detail_fields\n",
    "            \n",
    "            \n",
    "        elif n_type == 'TRANSFORMER':\n",
    "            \n",
    "            trans_expression = []\n",
    "            t_rows = get_trans_expression(con, meta, id)\n",
    "            \n",
    "            for t in t_rows:\n",
    "                trans_expression.append(t.expression)\n",
    "                \n",
    "            G.node[id]['transformer_expression'] = trans_expression\n",
    "            \n",
    "        elif n_type == 'AGGREGATOR':\n",
    "            \n",
    "            aggregator_expression = []\n",
    "            ae_rows = get_aggregator_expression(con, meta, id)\n",
    "            \n",
    "            for ae in ae_rows:\n",
    "                aggregator_expression.append(ae.expression)\n",
    "                \n",
    "            G.node[id]['aggregator_expression'] = aggregator_expression\n",
    "            \n",
    "            aggregator_group = []\n",
    "            ag_rows = get_aggregator_group(con, meta, id)\n",
    "            \n",
    "            for ag in ag_rows:\n",
    "                aggregator_group.append(ag.expression)\n",
    "                \n",
    "            G.node[id]['aggregator_group'] = aggregator_group\n",
    "            \n",
    "        elif n_type == 'SORT':\n",
    "            \n",
    "            sort_condition = []\n",
    "            \n",
    "            s_rows = get_sort_condition(con, meta, id)\n",
    "            \n",
    "            for s in s_rows:\n",
    "                sort_condition.append([[s.sort_field],[s.sort_direction],[s.idx]])\n",
    "                \n",
    "            G.node[id]['sort_condition'] = sort_condition\n",
    "            \n",
    "        elif n_type == 'FILTER':\n",
    "            \n",
    "            filter_expression =  row.expression\n",
    "            \n",
    "            G.node[id]['filter_expression'] = filter_expression\n",
    "            \n",
    "        elif n_type == 'UNION':\n",
    "            \n",
    "            G.node[id]['union_type'] = row.mode\n",
    "            \n",
    "        elif n_type in ('ROUTER'):\n",
    "            \n",
    "            G.node[id]['mode'] = row.mode\n",
    "            \n",
    "            exp_rows = get_router_expressions(con, meta, id)\n",
    "            \n",
    "            router_expressions = []\n",
    "            \n",
    "            for e in exp_rows:\n",
    "                router_expressions.append((e.expression))\n",
    "\n",
    "            G.node[id]['router_expressions'] = router_expressions\n",
    "            \n",
    "        elif n_type in ('COMPARE'):\n",
    "            \n",
    "            print(\">>> CDC COMPARE found\")\n",
    "            \n",
    "            j_rows = get_compare_join(con, meta, id)\n",
    "            \n",
    "            join_condition = []\n",
    "            \n",
    "            for j in j_rows:\n",
    "                idx = j.idx\n",
    "                join_condition.append((j.master_expression, j.join_operator, j.detail_expression))\n",
    "            \n",
    "            G.node[id]['join_condition'] = join_condition\n",
    "            \n",
    "            compare_fields = []\n",
    "            \n",
    "            c_rows = get_compare_condition(con, meta, id)\n",
    "            \n",
    "            for c in c_rows:\n",
    "                compare_fields.append(((c.master_expression, c.join_operator, c.detail_expression)))\n",
    "             \n",
    "            G.node[id]['compare_fields'] = compare_fields\n",
    "                \n",
    "            #detail_fields = []\n",
    "            \n",
    "            #d_rows = get_join_detail(con, meta, id)\n",
    "            \n",
    "            #for d in d_rows:\n",
    "            #    detail_fields.append((d.field_name, d.idx))\n",
    "             \n",
    "            #G.node[id]['detail_fields'] = detail_fields\n",
    "            \n",
    "        elif n_type in ('CUSTOM'):\n",
    "            \n",
    "            print('>>> CUSTOM Transform Found')\n",
    "            \n",
    "            G.node[id]['expression'] = row.expression\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\">>> Unexpected Package Tpe: {} - Found\".format(n_type))\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_tables = ('pd_bundle',\n",
    "               'pd_bundle_gui_state',\n",
    "               'pd_prep_package',\n",
    "               'pd_prep_connector',\n",
    "               'pd_prep_port',\n",
    "               'pd_prep_aggregator_expression',\n",
    "               'pd_prep_aggregator_group',\n",
    "               'pd_prep_compare_condition',\n",
    "               'pd_prep_compare_join',\n",
    "               'pd_prep_field_format',\n",
    "               'pd_prep_joiner_condition',\n",
    "               'pd_prep_joiner_detail_field',\n",
    "               'pd_prep_joiner_master_field',\n",
    "               'pd_prep_parameter', \n",
    "               'pd_prep_record_format',\n",
    "               'pd_prep_router_expression',\n",
    "               'pd_prep_sort_condition',\n",
    "               'pd_prep_transformer_expression',\n",
    "               'pd_entity',\n",
    "               'pd_source',\n",
    "               'pd_prepare_execution_workorder',\n",
    "               'pd_workorder'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con, meta = connect('je70', '2tb;U;{[>4i34&x', 'podium_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# host='cl11148.sunlifecorp.com'\n",
    "con, meta = connect('je70', 'wharf3dal3', 'podium_md', host='pg_dev5436.sunlifecorp.com', port=5436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.reflect(bind=con, schema='podium_core', only=prep_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in meta.tables:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle(con, meta, sname, world_graph):\n",
    "    \"\"\"Build bundle digraph and write to file\"\"\"\n",
    "    \n",
    "    bundle = get_bundle_id(con, meta, sname)\n",
    "    \n",
    "    print(bundle)\n",
    "    \n",
    "    if bundle:\n",
    "        \n",
    "        bundle_nid = bundle.nid\n",
    "        bundle_description = bundle.description\n",
    "        bundle_sname = bundle.sname\n",
    "        \n",
    "        bundle_gui_state = get_bundle_gui_state(con, meta, bundle.bundle_gui_state_nid)\n",
    "        \n",
    "        bundle_mod_dt = bundle_gui_state.modified_ttz\n",
    "        bundle_mod_by = bundle_gui_state.modifiedby\n",
    "        bundle_version = bundle_gui_state.version\n",
    "        \n",
    "        # To-do - check if output file for version already exists\n",
    "        #         if so then bypass\n",
    "        \n",
    "        bundle_exec = get_bundle_last_execution(con, meta, bundle_nid)\n",
    "        \n",
    "        if bundle_exec:\n",
    "            \n",
    "            exec_stats = []\n",
    "            \n",
    "            for i, r in enumerate(bundle_exec):\n",
    "                if i == 0:\n",
    "                    last_record_count = r.record_count\n",
    "                    last_start_time = r.start_time\n",
    "                    last_end_time = r.end_time\n",
    "                    \n",
    "                exec_stats.append(({'start_time': str(r.start_time), \n",
    "                                    'end_time': str(r.end_time),\n",
    "                                    'records': r.record_count}))\n",
    "                \n",
    "        else:\n",
    "            last_record_count = 0\n",
    "            last_start_time = ''\n",
    "            last_end_time = ''\n",
    "        \n",
    "        print(\"{}, {}, {} records {}\".format(bundle_nid, bundle_description, bundle_sname, last_record_count))\n",
    "        print(\"Modified by: {}, Modified Date: {}, Version: {}\".format(bundle_mod_by, bundle_mod_dt, bundle_version))\n",
    "        print(\"{}, {}\".format(last_start_time, last_end_time))\n",
    "        \n",
    "    else:\n",
    "        print(\"Package: {}, not found\".format(sname))\n",
    "        return None\n",
    "    \n",
    "    # add bundle to \"world\" graph\n",
    "    bundle_node_key = f'b_{bundle_nid}'\n",
    "    \n",
    "    W.add_node(bundle_node_key,\n",
    "               nid=bundle_nid, \n",
    "               sname=bundle_sname, \n",
    "               n_type='bundle')\n",
    "    \n",
    "    # Create Networkx DiGraph\n",
    "    G = nx.DiGraph(name=bundle_sname, \n",
    "                   id=bundle_nid) \n",
    "    \n",
    "    # Add nodes\n",
    "    p = get_package_nodes(con, meta, bundle_nid)\n",
    "    \n",
    "    # Add LOADER and STORE nodes to world map\n",
    "    for n in p:\n",
    "        \n",
    "        id = n.nid\n",
    "        n_type = n.package_type\n",
    "     \n",
    "        if n_type in ('LOADER','STORE'):\n",
    "            \n",
    "            entity_id = n.entity_id\n",
    "            \n",
    "            entity_node_key = f'e_{entity_id}'\n",
    "            \n",
    "            if (not W.has_node(entity_node_key)):\n",
    "            \n",
    "                entity = get_entity(con, meta, entity_id)\n",
    "                \n",
    "                source_id = entity.source_nid\n",
    "                source = get_source(con, meta, source_id)\n",
    "                \n",
    "                W.add_node(entity_node_key,\n",
    "                           n_type='entity',\n",
    "                           nid=entity_id,\n",
    "                           snid=source_id,\n",
    "                           sname=entity.sname)\n",
    "                \n",
    "                source_node_key = f's_{source_id}'\n",
    "                \n",
    "                if (not W.has_node(source_node_key)):\n",
    "                    \n",
    "                    W.add_node(source_node_key,\n",
    "                           n_type='source',\n",
    "                           nid=source_id,\n",
    "                           sname=source.sname)\n",
    "                    \n",
    "                W.add_edge(source_node_key, entity_node_key)\n",
    "                \n",
    "                if (n_type == 'STORE'):                    \n",
    "                    W.add_edge(bundle_node_key, entity_node_key)\n",
    "                elif (n_type == 'LOADER'):\n",
    "                    W.add_edge(entity_node_key, bundle_node_key)\n",
    "                else:\n",
    "                    print(f'ERROR {bundle_node_key}, {source_node_key}, {entity_node_key}')\n",
    "                \n",
    "            else:\n",
    "                source_nid = W.node[entity_node_key]['snid']\n",
    "                source_node_key = f's_{source_nid}'\n",
    "                print(f\"World map already has entity {entity_id}, {W.node[source_node_key]['sname']}.{W.node[entity_node_key]['sname']}\")\n",
    "                # print(f\"World map already has entity {entity_id}\")\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    # Resume bundle graph\n",
    "    \n",
    "    print(\"{} nodes in bundle\".format(len(p)))\n",
    "    \n",
    "    create_graph_nodes(con, meta, G, p)\n",
    "    \n",
    "    # Add Edges\n",
    "    c = get_package_connectors(con, meta, bundle_nid, G)\n",
    "    \n",
    "    for e in c:\n",
    "        G.add_edge(e[0], e[1], ob_idx=e[2], inbound_type=e[3], ib_idx=e[4], outbound_type=e[5])\n",
    "        \n",
    "    # Get the target  STORE, this will get level=1 attribute\n",
    "    # level used by visjs Network for drawing a hierarchy\n",
    "    for n in G.nodes():\n",
    "        n_type = G.node[n]['n_type']\n",
    "        if n_type == 'STORE':\n",
    "            target_node = G.node[n]\n",
    "            break\n",
    "        \n",
    "    print(\"Target node is {}\".format(n))\n",
    "\n",
    "    # Set levels for all other nodes\n",
    "    n_level = 0\n",
    "\n",
    "    target_node = n\n",
    "    \n",
    "    free = make_free()\n",
    "    \n",
    "    set_level(G, target_node, free, 0, 1)\n",
    "    \n",
    "    # At this point the levels are max -> 0\n",
    "    # so now invert\n",
    "    max_level = 0\n",
    "    \n",
    "    print(\"Longest path by dag_longest_path_length: {}\".format(nx.algorithms.dag.dag_longest_path_length(G)))\n",
    "\n",
    "    for n in G.nodes():\n",
    "        level = G.node[n]['level']\n",
    "        if level > max_level:\n",
    "            max_level = level\n",
    "\n",
    "    print(\"max level: {}\".format(max_level))\n",
    "    \n",
    "    for n in G.nodes():\n",
    "        new_level = max_level - G.node[n]['level']\n",
    "        G.node[n]['level'] = new_level \n",
    "     \n",
    "    # Prepare output file name\n",
    "    time_stamp = '{:%Y-%m-%d %H:%M}'.format(dt.datetime.today())\n",
    "\n",
    "    # Contruct dataflow object\n",
    "    df_obj = {\"workflow\":sname,\n",
    "              \"description\": bundle_description,\n",
    "              \"extractdate\": time_stamp,\n",
    "              \"bundle_id\": bundle_nid,\n",
    "              \"exec_stats\": exec_stats,\n",
    "              \"last_record_count\": last_record_count,\n",
    "              \"last_start_time\": str(last_start_time),\n",
    "              \"last_end_time\": str(last_end_time),\n",
    "              \"modified_dt\": str(bundle_mod_dt),\n",
    "              \"modified_by\": bundle_mod_by,\n",
    "              \"version\": str(bundle_version),\n",
    "              \"nodes\":[],\n",
    "              \"edges\":[]}\n",
    "\n",
    "    # save the router expressions for including in\n",
    "    # visjs edge label\n",
    "    \n",
    "    # This dictionary willl hold any ROUTERs expressions in the dataflow\n",
    "    # keyed by ROUTER package nid\n",
    "    router_exp = {}\n",
    "    \n",
    "    for n in G.nodes():\n",
    "\n",
    "        r = {\"id\":n}\n",
    "\n",
    "        for k in G.node[n]:\n",
    "            \n",
    "            kv = G.node[n][k]\n",
    "            r[k] = kv\n",
    "            \n",
    "            if k == 'router_expressions':\n",
    "                router_exp[n] = kv\n",
    "\n",
    "        df_obj['nodes'].append(r)\n",
    "\n",
    "    \n",
    "    # Joiner label, Master == left, Detail == right\n",
    "    joiner_master_detail_lbl = ('Master','Detail')\n",
    "    cdc_slave_master_lbl = ('Slave','Master')\n",
    "\n",
    "\n",
    "    # Build the visjs edge labels\n",
    "    for id, e in enumerate(G.edges(data=True)):\n",
    "\n",
    "        # Could be multiple labels so build as a list\n",
    "        label_txt = []\n",
    "        \n",
    "        node_from = e[0]\n",
    "        node_to = e[1]\n",
    "        \n",
    "        edge_metadata = e[2]\n",
    "\n",
    "        ob_idx= edge_metadata['ob_idx']\n",
    "        ib_idx= edge_metadata['ib_idx']\n",
    "        \n",
    "        target_type = edge_metadata['outbound_type']\n",
    "        source_type = edge_metadata['inbound_type']\n",
    "\n",
    "        if target_type == 'JOINER':\n",
    "            label_txt.append(joiner_master_detail_lbl[ib_idx])\n",
    "            \n",
    "        if target_type == 'COMPARE':\n",
    "            label_txt.append(cdc_slave_master_lbl[ib_idx])\n",
    "            \n",
    "        if source_type == 'ROUTER':\n",
    "            router_exp = G.node[node_from]['router_expressions']\n",
    "            label_txt.insert(0, router_exp[ob_idx])\n",
    "\n",
    "\n",
    "        df_obj['edges'].append({\"id\":id,\n",
    "                                \"from\":node_from,\n",
    "                                \"to\":node_to,\n",
    "                                \"label\":', '.join(label_txt).strip(),\n",
    "                                \"arrows\":\"to\"})\n",
    "        \n",
    "    file_time_stamp = '{:%Y_%m_%d_%H%M}'.format(dt.datetime.today())\n",
    "    \n",
    "    DIR = OUTPUT_DIR\n",
    "\n",
    "    filename = os.path.join(DIR, \"{}_{}.js\".format(sname,file_time_stamp))\n",
    "\n",
    "    fo = open(filename, 'w')\n",
    "\n",
    "    json.dump(df_obj, fo)\n",
    "\n",
    "    fo.close()\n",
    "\n",
    "    print(\"File written to {}\".format(filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sname = ['je70_oasis_delta_memberd_history_v','je70_oasis_init_memberd_history_v','je70_oasis_pre_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bundle_count = len(sname)\n",
    "\n",
    "# M = nx.DiGraph()\n",
    "W = nx.DiGraph()\n",
    "\n",
    "for i,s in enumerate(sname):\n",
    "    print(\"{} / {} - {}\".format(i+1, bundle_count, s))\n",
    "    get_bundle(con, meta, s, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
